\section{The near future: Noisy intermediate-scale quantum technology (NISQ)}\index{Noisy intermediate-scale quantum technology (NISQ)}\label{sec:NISQ}

\sectionby{Zixin Huang}\index{Zixin Huang}

\famousquote{The future belongs to those who believe in the beauty of their dreams.}{Eleanor Roosevelt}
\newline

\dropcap{I}{n} the near- to medium-term we are unlikely to make sufficient technological advances to realise fully scalable, fault-tolerant, universal quantum computation. But that doesn't mean we will have no quantum capabilities at all! Noisy intermediate-scale quantum technology (NISQ) refers to quantum processors which may be available in the next few years, with around 50 to a few hundred qubits. These are going to be noisy and will not have full quantum error-correcting capabilities. They are likely to be special-purpose devices targeted at specific applications, possibly yielding only approximate answers owing to the absence of fault-tolerance \cite{bib:preskill2018quantum}.

Although fully universal, fault-tolerant quantum computers are still somewhat distant, with advances in quantum control, we are now in the position to explore a new frontier of physics, where we have quantum entanglement as part of our computational toolbox. We may not yet have \textit{all} quantum capabilities, but we at least have some!

Scalable quantum computers, unlike classical ones, will be able to efficiently simulate any process that physically occurs in nature, enabling us to study the properties of complex molecules and new materials. This confidence is based on quantum complexity arguments\index{Computational!Complexity}, and our eventual capabilities to perform quantum error correction (which is admittedly very challenging and a potentially long-term vision). Both are based on quantum entanglement, a type of correlation between systems uniquely quantum mechanical, with no classical analogue. We have strong evidence that quantum computers have capabilities beyond classical computation. To illustrate this, consider the following:

\begin{itemize}
\item Quantum complexity: we have strong reason to believe that some tasks efficient on quantum computers may be computationally difficult classically. The best-known example is Shor's algorithm \cite{bib:ShorFactor}, allowing us to factorise large numbers exponentially  faster than using the best classical methods. Whilst we do not have a proof that an efficient classical algorithm doesn't exist, the brightest of mathematicians have been trying to find one for decades to no avail. Integer factorisation has significant implications for cryptography, where the security of some codes is underpinned by the believed computational complexity of this particular problem. 
\item Complexity theory arguments: computer scientists have shown that quantum states which can be easily prepared with a quantum computer have super-classical properties. For example, given single photons input into a multi-mode interferometer, it's hard for a classical computer to sample the probability distribution at the output, the so-called \textsc{BosonSampling} problem\index{Boson-sampling}. On the other hand, a quantum computer can trivially implement this experiment.
\item No known classical algorithm can efficiently simulate a universal, fault-tolerant quantum computer, or simulate general quantum systems.
\end{itemize}

As we see, there is a clear distinction between what is hard classically and quantum mechanically. Intense research efforts are being dedicated to understanding which problems exactly are hard for a classical computer but easy for a quantum one.

The huge obstacle that lies between us and building a scalable quantum computer is the need to keep the system isolated from the environment to minimise noise (environmental noise is the arch-enemy of quantum computation!), at the same time being able to control it with extraordinary precision. Eventually, we expect to be able to protect quantum systems using quantum error correction. However, in order to perform quantum error correction, we currently believe that perhaps $10^3$-$10^4$ physical qubits will be required to encode each logical qubit (depending on the physical architecture and its associated error rates). This adds huge overheads to the number of physical qubits needing to be individually prepared, manipulated and measured, all with extremely high fidelity. Therefore, reliable fault-tolerant quantum computers with quantum error correction are not likely going to be available in the near future.

In terms of the number of qubits, 50 is a significant number because it approximates the number of qubits we can still simulate by brute-force\index{Brute-force} with our most powerful existing classical computers \cite{bib:boixo2018characterizing} -- a benchmark for the meaning of the term \textit{quantum supremacy}\index{Quantum supremacy}. The main question is: when will quantum computers be able to solve useful problems faster than classical ones? This leads us onto several potential uses for limited quantum computation in the NISQ era:

\subsection{Quantum optimisers}\index{Quantum optimisation}

For many problems, there is a big gap between the approximation achieved by classical algorithms and the barrier of exact-case \textbf{NP}-hardness. We do not expect quantum computers to efficiently solve worst-case \textbf{NP}-hard problems, however, quantum devices may be able to find better \textit{approximate} solutions to such problems, or at least find such approximations more quickly. The vision for using NISQ to solve optimisation problems is a hybrid quantum-classical algorithm\index{Hybrid!Algorithms}. In this scheme we use the quantum device to produce and manipulate an $n$-qubit state, measure the qubits, then process the measurement outcomes classically. This then is utilised as feedback for the next round of quantum state preparation and evolution. The cycle is repeated until convergence is obtained to a quantum state from which the approximate answer can be extracted. Two such algorithms are known as \textit{quantum approximate optimisation algorithms} \cite{bib:farhi2014quantum}\index{Quantum approximate optimisation algorithms}, and \textit{variational quantum eigensolvers} \cite{bib:mcclean2016theory}\index{Variational quantum eigensolvers}.

\subsection{Quantum machine learning}\index{Quantum machine learning}

\famousquote{Many do not lose their mind because they do not have one.}{Arthur Schopenhauer}
\newline

Much of the quantum machine learning (QML) literature builds on algorithms which speed up problems in linear algebra \cite{bib:biamonte2017quantum}. One of the potentials for QML rests upon QRAM -- quantum random-access memory\index{Quantum random-access memory (QRAM)}. For classical data processing, by using QRAM we may be able to represent a large amount of classical data, $N$-bits, using only $O(\log N)$ qubits, an exponential improvement in resource efficiency. However, the bottleneck may be in the encoding/decoding of the QRAM, which may seemingly mitigate potential gains, owing to the fact that measurements yield only one element at a time, not the full exponentially-large structure. QML may find applications in a more natural setting where both the input an output are quantum states, for example, to control a quantum system, or in learning probability distributions where entanglement plays an important role.

\subsection{Quantum semidefinite programming}\index{Quantum semidefinite programming}

Semidefinite programming is the task of optimising a linear function, given some matrix inequality constraints. Classically, the problem can be solved in time polynomial in matrix size, and the number of constraints.

A quantum algorithm has been shown to find an approximate solution to this problem with an exponential speedup \cite{bib:brandao2017quantum, bib:brandao2017exponential}. In this algorithm, the initial state is a thermal state\index{Thermal!States} that is a function of the input matrices for the semidefinite program. The success of the implementation depends on whether the particular thermal state can be efficiently prepared. The output is a quantum state, which approximates the optimal matrix. The quantum state can be measured to extract (via sampling) features of this matrix. 

The crucial feature in the quantum algorithm is the preparation of a thermal state of non-zero temperature, suggesting the algorithm may be intrinsically robust against thermal noise -- this would be a fantastic trait to exhibit in the NISQ era of no fault-tolerance. It's therefore entirely possible that a quantum solver for semidefinite programs might be achievable with near-term NISQ technology.

\subsection{Quantum dynamics}\index{Quantum dynamics}

As was stressed previously, quantum computers are very well suited to studying highly entangled, multi-particle systems. It's the natural platform to simulate entangled states, where quantum computers appear to have a clear intrinsic advantage over classical ones.
 
With a universal quantum computer, we anticipate that studying quantum chemistry\index{Quantum chemistry} (especially noisy quantum chemistry) will be enabled. Ideally, if the noise model in the quantum computer can be cleverly mapped to be isomorphic to the noise present in the physical system being simulated, then noise becomes a feature not a bug! This could be used in the design of new pharmaceuticals\index{Drug!Design}, for example, as well as catalysts for improving the efficiency of nitrogen fixation\index{Nitrogen fixation} or carbon capture\index{Carbon capture}. We may be able to find new materials with better resistive properties, leading to more efficient transmission of electricity. However, these promises may not be fulfilled with NISQ, because algorithms to accurately simulate large molecules and materials may not succeed without quantum error correction.

We do know that classical computers are particularly inefficient at simulating quantum dynamics, i.e how highly entangled quantum states will evolve over time. Here quantum computers have a particularly obvious advantage, and one example would be quantum chaos\index{Quantum chaos}. In these systems entanglement spreads very rapidly. Insights might be gained using noisy devices on the order of only 100's of qubits, a perfect regime for the NISQ era.

We've barely had a glimpse of the promises of NISQ. But it's clear that although near-term devices will be limited, they may nonetheless open up exciting new prospects and computational applications, beyond the capabilities of present-day classical machinery.